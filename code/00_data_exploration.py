# -*- coding: utf-8 -*-
"""00_Data_Exploration_Refactored.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1slgMdFw6jo7YMJVSXnKdZqckYa_nE_uo

# Data Exploration and Time Series Analysis

## Overview
This notebook serves as the foundational step in the sales forecasting project. The primary objective is to inspect the raw daily sales data, understand its underlying distribution, and prepare it for statistical modeling (specifically SARIMA).

## Key Objectives
1.  **Data Quality Check**: Inspect data structure, handle outliers, and filter relevant transaction types.
2.  **Univariate Analysis**: Analyze the distribution of sales quantities using histograms and standard deviation thresholds.
3.  **Time Series Aggregation**: Convert transactional data into a monthly time series indexed by accounting periods.
4.  **Signal Decomposition**: Separate gross sales (positive) from returns (negative) to analyze distinct behaviors.
5.  **Stationarity Analysis**: Apply transformations (Log, Differencing) and statistical tests (ADF, KPSS) to ensure the series satisfies ARIMA assumptions.
6.  **Correlation Analysis**: Examine ACF and PACF plots to determine potential autoregressive (AR) and moving average (MA) orders.

## 1. Environment Setup
Importing necessary libraries and configuring the environment.
"""

import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller, kpss
from statsmodels.tsa.stattools import pacf as sm_pacf

# Configure plotting style
plt.style.use('seaborn-v0_8-whitegrid')

# Define dataset path
DAILY_SALES_PATH = "../dataset/dataset_daily_sales.csv"

"""## 2. Utility Functions
Helper functions for normalization, statistical calculations, and transformations.
"""

def solve_linear_system(A, b):
    A = [row[:] for row in A]
    b = list(b)
    n = len(A)

    for i in range(n):
        pivot = A[i][i]
        if pivot == 0:
            raise ValueError("Zero pivot encountered")

        for j in range(i, n):
            A[i][j] /= pivot
        b[i] /= pivot

        for k in range(i + 1, n):
            factor = A[k][i]
            for j in range(i, n):
                A[k][j] -= factor * A[i][j]
            b[k] -= factor * b[i]

    x = [0.0] * n
    for i in range(n - 1, -1, -1):
        s = sum(A[i][j] * x[j] for j in range(i + 1, n))
        x[i] = b[i] - s
    return x

def normalize_series(series):
    """Standardizes a pandas Series (Z-score normalization)."""
    mean_val = series.mean()
    std_dev = series.std()
    return (series - mean_val) / std_dev

def calculate_acf(series, n_lags):
    """Calculates the Autocorrelation Function (ACF) manually."""
    acf_values = []
    for lag in range(n_lags + 1):
        if lag == 0:
            acf_values.append(1.0)
        else:
            # Compute correlation with shifted series
            correlation = series.corr(series.shift(lag))
            acf_values.append(correlation)
    return pd.Series(acf_values, index=range(n_lags + 1))

def variance_stabilizing_log(series):
    """Applies log transformation, shifting data if necessary to handle non-positive values."""
    s = series.copy().astype(float)
    min_val = s.min()

    # Shift series to ensure all values are positive before log
    if min_val <= 0:
        shift = 1.0 - min_val
        s = s + shift

    return s.apply(lambda x: math.log(x))

def calculate_pacf_safe(series, nlags):
    s = series.dropna().astype(float)
    nlags = min(nlags, len(s) - 1)
    return sm_pacf(s, nlags=nlags, method="ywm")

"""## 3. Data Loading and Cleaning
Load the dataset and filter for relevant sales transactions.
"""

# Load dataset with low_memory=False to handle mixed types warning
daily_sales_df = pd.read_csv(DAILY_SALES_PATH, low_memory=False)

# Filter for 'Sales Units' and remove zero-amount records
daily_sales_df = daily_sales_df[
    (daily_sales_df['Account'] == 'Sales Units') &
    (daily_sales_df['Amount'] != 0)
]

# Display basic structure
print(f"Total Records after filtering: {len(daily_sales_df)}")
display(daily_sales_df.head(3))

# Verify column names and unique countries for context
print("Columns:", daily_sales_df.columns.tolist())
display(daily_sales_df["ISO_COUNTRY_NAME_ENGLISH"].drop_duplicates().head())
display(daily_sales_df["PRODUCT_STEERING_L5_NAME"].drop_duplicates())

# Sort by date components to ensure chronological order
date_cols = ['DOCUMENT_DATE', 'ACCOUNTING_PERIOD', 'Year', 'Month', 'month_name', 'Day']
sorted_dates_df = daily_sales_df[date_cols].drop_duplicates().sort_values(by=['Year', 'Month', 'Day'])
display(sorted_dates_df.head())

"""## 4. Distribution Analysis
Analyzing the statistical spread of sales amounts and handling outliers.
"""

# Descriptive statistics
display(daily_sales_df["Amount"].describe())

"""### 4.1 Histograms
Visualizing the data distribution including outliers.
"""

plt.figure(figsize=(10, 6))
plt.hist(daily_sales_df['Amount'], bins=200, edgecolor='black', alpha=0.7)
plt.title('Distribution of Sales Amount (Raw Data)')
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.5)
plt.show()

"""### 4.2 Outlier Filtering (3-Sigma Rule)
Filtering data to focus on the core distribution (within 3 standard deviations).
"""

mean_amount = daily_sales_df['Amount'].mean()
std_amount = daily_sales_df['Amount'].std()

# Define 3-sigma boundaries
lower_bound_3s = mean_amount - (3 * std_amount)
upper_bound_3s = mean_amount + (3 * std_amount)

df_filtered_3s = daily_sales_df[
    (daily_sales_df['Amount'] >= lower_bound_3s) &
    (daily_sales_df['Amount'] <= upper_bound_3s)
]

# Dynamic bin count based on log rule
n_3s = len(df_filtered_3s['Amount'])
bins_3s = round(1 + math.log2(n_3s))

plt.figure(figsize=(10, 6))
plt.hist(df_filtered_3s['Amount'], bins=bins_3s, edgecolor='black', alpha=0.7)
plt.title('Distribution of Sales Amount (within 3 std deviations)')
plt.xlabel('Amount')
plt.ylabel('Frequency')
plt.show()

"""### 4.3 Outlier Filtering (2-Sigma Rule)
A tighter filter (within 2 standard deviations).
"""

# Define 2-sigma boundaries
lower_bound_2s = mean_amount - (2 * std_amount)
upper_bound_2s = mean_amount + (2 * std_amount)

df_filtered_2s = daily_sales_df[
    (daily_sales_df['Amount'] >= lower_bound_2s) &
    (daily_sales_df['Amount'] <= upper_bound_2s)
]

n_2s = len(df_filtered_2s['Amount'])
bins_2s = round(1 + math.log2(n_2s))

plt.figure(figsize=(10, 6))
plt.hist(df_filtered_2s['Amount'], bins=bins_2s, edgecolor='black', alpha=0.7)
plt.title('Distribution of Sales Amount (within 2 std deviations)')
plt.xlabel('Amount')
plt.show()

# Identify most frequent specific sales amounts
top_10_amounts = daily_sales_df['Amount'].value_counts().head(10).reset_index()
top_10_amounts.columns = ['Amount Value', 'Frequency']
display(top_10_amounts)

"""## 5. Time Series Aggregation
Aggregating data by Accounting Period to create a monthly time series.
"""

# 5.1 Total Sales (Net)
amount_over_time = daily_sales_df.groupby('ACCOUNTING_PERIOD')['Amount'].sum().reset_index()
amount_over_time = amount_over_time.sort_values(by='ACCOUNTING_PERIOD')
amount_over_time['ACCOUNTING_PERIOD'] = amount_over_time['ACCOUNTING_PERIOD'].astype(str)

plt.figure(figsize=(12, 6))
plt.plot(amount_over_time['ACCOUNTING_PERIOD'], amount_over_time['Amount'], marker='o')
plt.xlabel('Accounting Period')
plt.ylabel('Net Sales Units')
plt.title('Total Sales Over Time')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# 5.2 Positive Sales Only (Gross Sales)
positive_amounts_df = daily_sales_df[daily_sales_df['Amount'] > 0]
amount_over_time_pos = positive_amounts_df.groupby('ACCOUNTING_PERIOD')['Amount'].sum().reset_index()
amount_over_time_pos = amount_over_time_pos.sort_values(by='ACCOUNTING_PERIOD')
amount_over_time_pos['ACCOUNTING_PERIOD'] = amount_over_time_pos['ACCOUNTING_PERIOD'].astype(str)

plt.figure(figsize=(12, 6))
plt.plot(amount_over_time_pos['ACCOUNTING_PERIOD'], amount_over_time_pos['Amount'], marker='o', color='green')
plt.title('Total Positive Sales (Gross)')
plt.xticks(rotation=45, ha='right')
plt.show()

# 5.3 Returns Only (Negative Amounts)
negative_amounts_df = daily_sales_df[daily_sales_df['Amount'] < 0]
amount_over_time_neg = negative_amounts_df.groupby('ACCOUNTING_PERIOD')['Amount'].sum().abs().reset_index()
amount_over_time_neg = amount_over_time_neg.sort_values(by='ACCOUNTING_PERIOD')
amount_over_time_neg['ACCOUNTING_PERIOD'] = amount_over_time_neg['ACCOUNTING_PERIOD'].astype(str)

plt.figure(figsize=(12, 6))
plt.plot(amount_over_time_neg['ACCOUNTING_PERIOD'], amount_over_time_neg['Amount'], marker='o', color='red')
plt.title('Total Returns (Absolute Value)')
plt.xticks(rotation=45, ha='right')
plt.show()

"""### 5.4 Comparison: Sales vs. Returns
Overlaying Gross Sales and Returns to observe correlations.
"""

plt.figure(figsize=(14, 7))

# Plot Gross Sales
plt.plot(amount_over_time_pos['ACCOUNTING_PERIOD'],
         amount_over_time_pos['Amount'],
         marker='o', color='green', label='Sales Units')

# Plot Returns (Absolute)
plt.plot(amount_over_time_neg['ACCOUNTING_PERIOD'],
         amount_over_time_neg['Amount'].abs(),
         marker='x', color='red', linestyle='--', label='Returned Units')

plt.title('Sales Units vs. Returned Units')
plt.xlabel('Accounting Period')
plt.ylabel('Units')
plt.xticks(rotation=45, ha='right')
plt.legend()
plt.tight_layout()
plt.show()

"""## 6. Autocorrelation (ACF) Analysis
Examining series memory and seasonality. We normalize the series first.
"""

n_lags = 12

# Normalize series
norm_pos_sales_ts = normalize_series(amount_over_time_pos["Amount"])
norm_neg_sales_ts = normalize_series(amount_over_time_neg["Amount"])

# Fig 1: ACF for Normalized Gross Sales
acf_pos = calculate_acf(norm_pos_sales_ts, n_lags)

plt.figure(figsize=(12, 6))
plt.stem(acf_pos.index, acf_pos.values)
plt.title('Fig 1: ACF for Normalized Sales Units (Gross)')
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.show()

# Fig 2: ACF for Normalized Returns
acf_neg = calculate_acf(norm_neg_sales_ts, n_lags)

plt.figure(figsize=(12, 6))
plt.stem(acf_neg.index, acf_neg.values)
plt.title('Fig 2: ACF for Normalized Returned Units')
plt.show()

"""### 6.1 Differenced ACF
Checking ACF after first-order differencing to remove trend.
"""

# Fig 3: ACF for Differenced Gross Sales
diff_pos_sales = norm_pos_sales_ts.diff(1).dropna()
acf_diff_pos = calculate_acf(diff_pos_sales, n_lags)

plt.figure(figsize=(12, 6))
plt.stem(acf_diff_pos.index, acf_diff_pos.values)
plt.title('Fig 3: ACF for Differenced Sales (Gross)')
plt.show()

# Fig 4: ACF for Differenced Returns
diff_neg_sales = norm_neg_sales_ts.diff(1).dropna()
acf_diff_neg = calculate_acf(diff_neg_sales, n_lags)

plt.figure(figsize=(12, 6))
plt.stem(acf_diff_neg.index, acf_diff_neg.values)
plt.title('Fig 4: ACF for Differenced Returns')
plt.show()

"""## 7. Stationarity and Transformations
Preparing the Net Sales series for SARIMA modeling. We apply Log transformation to stabilize variance, followed by differencing.
"""

# Fig 5: ACF for Net Sales (Base)
agg_sales_ts = amount_over_time.sort_values(by='ACCOUNTING_PERIOD')["Amount"]
acf_agg = calculate_acf(agg_sales_ts, n_lags)

plt.figure(figsize=(12, 6))
plt.stem(acf_agg.index, acf_agg.values)
plt.title('Fig 5: ACF for Net Sales Units (Base)')
plt.show()

"""### 7.1 Log Transformation & First Difference
Applying Log to handle variance and Diff(1) to remove trend.
"""

agg_sales_ts = amount_over_time.sort_values(by='ACCOUNTING_PERIOD')["Amount"].astype(float)

# Variance stabilization (Log)
log_sales_ts = variance_stabilizing_log(agg_sales_ts)

# First difference
diff1_log_sales = log_sales_ts.diff(1).dropna()

# ACF Plot
acf_diff1_log = calculate_acf(diff1_log_sales, n_lags)

plt.figure(figsize=(12, 6))
plt.stem(acf_diff1_log.index, acf_diff1_log.values)
plt.title('Fig 6: ACF for diff(1) of Log(Net Sales)')
plt.show()

"""### 7.2 Seasonal Differencing (Lag 12)
Checking for seasonality by differencing at lag 12.
"""

# Seasonal difference (Log series)
diff12_log_sales = log_sales_ts.diff(12).dropna()
acf_diff12_log = calculate_acf(diff12_log_sales, n_lags)

plt.figure(figsize=(12, 6))
plt.stem(acf_diff12_log.index, acf_diff12_log.values)
plt.title('Fig 7: ACF for diff(12) of Log(Net Sales)')
plt.show()

# Plot the series itself
plt.figure(figsize=(12, 4))
plt.plot(diff12_log_sales.index, diff12_log_sales.values)
plt.title("Seasonal Difference (Lag 12) of Log Sales")
plt.xlabel("Index")
plt.ylabel("Value")
plt.show()

"""### 7.3 Statistical Tests (ADF & KPSS)
Testing the stationarity of the differenced series.
*   **ADF**: Null Hypothesis = Non-Stationary (Unit Root).
*   **KPSS**: Null Hypothesis = Stationary.
"""

series_to_test = diff12_log_sales.dropna()

# ADF Test
adf_stat, adf_p, _, _, adf_crit, _ = adfuller(series_to_test, autolag='AIC')
print("ADF Test Results:")
print(f"  Statistic: {adf_stat:.4f}")
print(f"  p-value: {adf_p:.4f}")
print(f"  Critical Values: {adf_crit}")

# KPSS Test
kpss_stat, kpss_p, _, kpss_crit = kpss(series_to_test, regression="c", nlags="auto")
print("\nKPSS Test Results:")
print(f"  Statistic: {kpss_stat:.4f}")
print(f"  p-value: {kpss_p:.4f}")
print(f"  Critical Values: {kpss_crit}")

"""### 7.4 Double Differencing (d=1, D=1)
Applying both trend and seasonal differencing to ensure robustness.
"""

# Apply first difference then seasonal difference
diff1_d1_log_sales = log_sales_ts.diff(1).diff(12).dropna()

print(f"Original Length: {len(log_sales_ts)}")
print(f"Length after Double Diff: {len(diff1_d1_log_sales)}")

plt.figure(figsize=(12, 4))
plt.plot(diff1_d1_log_sales.index, diff1_d1_log_sales.values)
plt.title("Double Differenced Series: diff(1) + diff(12) of Log Sales")
plt.show()

# Re-run statistical tests on double differenced data
series_to_test_dd = diff1_d1_log_sales.dropna()

adf_stat, adf_p, _, _, adf_crit, _ = adfuller(series_to_test_dd, autolag="AIC")
print("ADF Test (Double Diff):")
print(f"  Statistic: {adf_stat:.4f}")
print(f"  p-value: {adf_p:.4f}")

kpss_stat, kpss_p, _, kpss_crit = kpss(series_to_test_dd, regression="c", nlags="auto")
print("\nKPSS Test (Double Diff):")
print(f"  Statistic: {kpss_stat:.4f}")
print(f"  p-value: {kpss_p:.4f}")

"""## 8. Partial Autocorrelation (PACF)
Calculating PACF to help identify AR terms.
"""

pacf_vals = calculate_pacf_safe(diff1_d1_log_sales, n_lags)

plt.figure(figsize=(12, 6))
plt.stem(range(len(pacf_vals)), pacf_vals)
plt.title("Fig 8: PACF of Double Differenced Log Sales")
plt.xlabel("Lag")
plt.ylabel("Partial Autocorrelation")
plt.show()

"""## 9. ACF and PACF by BUSINESS_AGGREGATE_NAME
Evaluating d based on `Log + diff(1)` for each product aggregate.
"""

AGGREGATE_COLUMN = "CUST_STEERING_L3_NAME"

agg_by_ba = (
    daily_sales_df
    .groupby([AGGREGATE_COLUMN, "ACCOUNTING_PERIOD"], as_index=False)["Amount"]
    .sum()
)

agg_by_ba["ACCOUNTING_PERIOD"] = agg_by_ba["ACCOUNTING_PERIOD"].astype(str)
agg_by_ba = agg_by_ba.sort_values([AGGREGATE_COLUMN, "ACCOUNTING_PERIOD"])

unique_ba_names = agg_by_ba[AGGREGATE_COLUMN].dropna().unique()

for ba_name in unique_ba_names:
    ba_df = agg_by_ba[agg_by_ba[AGGREGATE_COLUMN] == ba_name].copy()

    ts = ba_df["Amount"].astype(float).reset_index(drop=True)


    log_ts = variance_stabilizing_log(ts)
    diff1_log_ts = log_ts.diff(1).dropna()

    if len(diff1_log_ts) <= 2:
        print(f"Skipping '{ba_name}': series too short after diff(1).")
        continue

    max_lags = 12

    acf_vals_ba = calculate_acf(diff1_log_ts, max_lags)
    pacf_vals_ba = calculate_pacf_safe(diff1_log_ts, max_lags)

    fig, axes = plt.subplots(1, 2, figsize=(10, 3))

    axes[0].stem(acf_vals_ba.index, acf_vals_ba.values)
    axes[0].set_title(f"ACF Log + diff(1)\n{ba_name}")
    axes[0].set_xlabel("Lag")
    axes[0].set_ylabel("Autocorrelation")

    axes[1].stem(range(len(pacf_vals_ba)), pacf_vals_ba)
    axes[1].set_title(f"PACF Log + diff(1)\n{ba_name}")
    axes[1].set_xlabel("Lag")
    axes[1].set_ylabel("Partial Autocorrelation")

    plt.tight_layout()
    plt.show()

AGGREGATE_COLUMN = "CUST_STEERING_L3_NAME"

agg_by_ba = (
    daily_sales_df
    .groupby([AGGREGATE_COLUMN, "ACCOUNTING_PERIOD"], as_index=False)["Amount"]
    .sum()
)

agg_by_ba["ACCOUNTING_PERIOD"] = agg_by_ba["ACCOUNTING_PERIOD"].astype(str)
agg_by_ba = agg_by_ba.sort_values([AGGREGATE_COLUMN, "ACCOUNTING_PERIOD"])

unique_ba_names = agg_by_ba[AGGREGATE_COLUMN].dropna().unique()

for ba_name in unique_ba_names:
    ba_df = agg_by_ba[agg_by_ba[AGGREGATE_COLUMN] == ba_name].copy()

    ts = ba_df["Amount"].astype(float).reset_index(drop=True)


    log_ts = variance_stabilizing_log(ts)
    diff1_log_ts = log_ts.diff(12).dropna()

    if len(diff1_log_ts) <= 2:
        print(f"Skipping '{ba_name}': series too short after diff(1).")
        continue

    max_lags = 6

    acf_vals_ba = calculate_acf(diff1_log_ts, max_lags)
    pacf_vals_ba = calculate_pacf_safe(diff1_log_ts, max_lags)

    fig, axes = plt.subplots(1, 2, figsize=(10, 3))

    axes[0].stem(acf_vals_ba.index, acf_vals_ba.values)
    axes[0].set_title(f"ACF Log + diff(1)\n{ba_name}")
    axes[0].set_xlabel("Lag")
    axes[0].set_ylabel("Autocorrelation")

    axes[1].stem(range(len(pacf_vals_ba)), pacf_vals_ba)
    axes[1].set_title(f"PACF Log + diff(1)\n{ba_name}")
    axes[1].set_xlabel("Lag")
    axes[1].set_ylabel("Partial Autocorrelation")

    plt.tight_layout()
    plt.show()